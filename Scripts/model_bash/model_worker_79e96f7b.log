2024-01-19 16:23:54 | INFO | model_worker | args: Namespace(host='localhost', port=31021, worker_address='http://localhost:31021', controller_address='http://localhost:21001', model_path='/PLMs/7b-chat', revision='main', device='cuda', gpus=None, num_gpus=8, max_gpu_memory='31Gib', dtype='float16', load_8bit=False, cpu_offloading=False, gptq_ckpt=None, gptq_wbits=16, gptq_groupsize=-1, gptq_act_order=False, awq_ckpt=None, awq_wbits=16, awq_groupsize=-1, enable_exllama=False, exllama_max_seq_len=4096, exllama_gpu_split=None, exllama_cache_8bit=False, enable_xft=False, xft_max_seq_len=4096, xft_dtype=None, model_names=['llama-2-7b-chat'], conv_template=None, embed_in_truncate=False, limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None, debug=False, ssl=False)
2024-01-19 16:23:54 | INFO | model_worker | Loading the model ['llama-2-7b-chat'] on worker 79e96f7b ...
2024-01-19 16:23:55 | ERROR | stderr | Traceback (most recent call last):
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/runpy.py", line 197, in _run_module_as_main
2024-01-19 16:23:55 | ERROR | stderr |     return _run_code(code, main_globals, None,
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/runpy.py", line 87, in _run_code
2024-01-19 16:23:55 | ERROR | stderr |     exec(code, run_globals)
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/fastchat/serve/model_worker.py", line 375, in <module>
2024-01-19 16:23:55 | ERROR | stderr |     args, worker = create_model_worker()
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/fastchat/serve/model_worker.py", line 346, in create_model_worker
2024-01-19 16:23:55 | ERROR | stderr |     worker = ModelWorker(
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/fastchat/serve/model_worker.py", line 77, in __init__
2024-01-19 16:23:55 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/fastchat/model/model_adapter.py", line 348, in load_model
2024-01-19 16:23:55 | ERROR | stderr |     model, tokenizer = adapter.load_model(model_path, kwargs)
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/fastchat/model/model_adapter.py", line 95, in load_model
2024-01-19 16:23:55 | ERROR | stderr |     model = AutoModelForCausalLM.from_pretrained(
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
2024-01-19 16:23:55 | ERROR | stderr |     return model_class.from_pretrained(
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3462, in from_pretrained
2024-01-19 16:23:55 | ERROR | stderr |     model = cls(config, *model_args, **model_kwargs)
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 1109, in __init__
2024-01-19 16:23:55 | ERROR | stderr |     self.model = LlamaModel(config)
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 956, in __init__
2024-01-19 16:23:55 | ERROR | stderr |     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 956, in <listcomp>
2024-01-19 16:23:55 | ERROR | stderr |     [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 756, in __init__
2024-01-19 16:23:55 | ERROR | stderr |     self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 319, in __init__
2024-01-19 16:23:55 | ERROR | stderr |     self._init_rope()
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 323, in _init_rope
2024-01-19 16:23:55 | ERROR | stderr |     self.rotary_emb = LlamaRotaryEmbedding(
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 134, in __init__
2024-01-19 16:23:55 | ERROR | stderr |     self._set_cos_sin_cache(
2024-01-19 16:23:55 | ERROR | stderr |   File "/root/anaconda3/envs/langchain/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py", line 146, in _set_cos_sin_cache
2024-01-19 16:23:55 | ERROR | stderr |     self.register_buffer("sin_cached", emb.sin().to(dtype), persistent=False)
2024-01-19 16:23:55 | ERROR | stderr | KeyboardInterrupt
